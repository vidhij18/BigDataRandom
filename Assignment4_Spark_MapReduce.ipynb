{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfstpKhUM8x7"
   },
   "source": [
    "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
    "<h2>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Assignment 4 - MapReduce and Spark</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWwndZSiM8x8"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise you is asked to use Spark for implementing an algorithm that applies computations on documents and dataframes.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font  size=\"3\" color='#91053d'>**Execute the following cell in order to initialize Spark**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3shtJGpOM8x8",
    "outputId": "8b100790-dcef-44a6-da62-524a60617907"
   },
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
    "# !tar zxvf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import of the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#inizialization of the Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder.master(\"local[*]\") \\\n",
    "    .appName(\"Assignment4\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK49MFw9M8yA"
   },
   "source": [
    "# Analysing documents\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We have already seen that MapReduce procedures are good in analyzing text-files.\n",
    "    \n",
    "The provided data comes from a scraping operation on the website https://www.vagalume.com.br/ and is available on kaggle:\n",
    "    \n",
    "https://www.kaggle.com/neisse\n",
    "    \n",
    "\n",
    "    \n",
    "The assignment is divided in 2 parts:\n",
    "    \n",
    "* Part 1 is focused on MapReduce \n",
    "    \n",
    "* Part 2  is focuses on dataframes\n",
    "    </font>\n",
    "    </p>\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font  size=\"3\" color='#91053d'>Notice that  dataset is noisy and shows all the typical issues related with data coming from this procedure (duplicated entries, etc).</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK49MFw9M8yA"
   },
   "source": [
    "# Part 1 -  MapReduce\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In the provided folder you can find a set of documents/files containing  descriptions of songs (lyrics and additional informations). Specifically in each file:\n",
    "\n",
    "- the first line is the idiom/language\n",
    "- the second line is the relative url of the song of the original website\n",
    "- the third line is the title of a song \n",
    "- from fourth line on the text you find the lyrics of the song.\n",
    "    </font>\n",
    "    </p>\n",
    "\n",
    "## Exercise 1 - (2 points) - Song's lyrics \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Provide a Spark MapReduce procedure that reads the documents and checks how many song's lyrics appear at least two times.\n",
    "\n",
    "In the data-interpretation of this exercise you can consider that two files represent the same lyric if the url (3rd line of each file) is the same.\n",
    "\n",
    " </font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font  size=\"3\" color='#91053d'>Notice that  you can reuse any code that was made available for the previous labs/assignments or that you already developed in these contexts.</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file ..  1.txt\n",
      "read file ..  10.txt\n",
      "read file ..  100.txt\n",
      "read file ..  1000.txt\n",
      "read file ..  10000.txt\n",
      "read file ..  100000.txt\n",
      "read file ..  100001.txt\n",
      "read file ..  100002.txt\n",
      "read file ..  100003.txt\n",
      "read file ..  100004.txt\n",
      "read file ..  100005.txt\n",
      "read file ..  100006.txt\n",
      "read file ..  100007.txt\n",
      "read file ..  100008.txt\n",
      "read file ..  100009.txt\n",
      "read file ..  10001.txt\n",
      "read file ..  100010.txt\n",
      "read file ..  100011.txt\n",
      "read file ..  100012.txt\n",
      "read file ..  100013.txt\n",
      "read file ..  100014.txt\n",
      "read file ..  100015.txt\n",
      "read file ..  100016.txt\n",
      "read file ..  100017.txt\n",
      "read file ..  100018.txt\n",
      "read file ..  100019.txt\n",
      "read file ..  10002.txt\n",
      "read file ..  100020.txt\n",
      "read file ..  100021.txt\n",
      "read file ..  100022.txt\n",
      "read file ..  100023.txt\n",
      "read file ..  100024.txt\n",
      "read file ..  100025.txt\n",
      "read file ..  100026.txt\n",
      "read file ..  100027.txt\n",
      "read file ..  100028.txt\n",
      "read file ..  100029.txt\n",
      "read file ..  10003.txt\n",
      "read file ..  100030.txt\n",
      "read file ..  100031.txt\n",
      "read file ..  100032.txt\n",
      "read file ..  100033.txt\n",
      "read file ..  100034.txt\n",
      "read file ..  100035.txt\n",
      "read file ..  100036.txt\n",
      "read file ..  100037.txt\n",
      "read file ..  100038.txt\n",
      "read file ..  100039.txt\n",
      "read file ..  10004.txt\n",
      "read file ..  100040.txt\n",
      "read file ..  100041.txt\n",
      "read file ..  100042.txt\n",
      "read file ..  100043.txt\n",
      "read file ..  100044.txt\n",
      "read file ..  100045.txt\n",
      "read file ..  100046.txt\n",
      "read file ..  100047.txt\n",
      "read file ..  100048.txt\n",
      "read file ..  100049.txt\n",
      "read file ..  10005.txt\n",
      "read file ..  100050.txt\n",
      "read file ..  100051.txt\n",
      "read file ..  100052.txt\n",
      "read file ..  100053.txt\n",
      "read file ..  100054.txt\n",
      "read file ..  100055.txt\n",
      "read file ..  100056.txt\n",
      "read file ..  100057.txt\n",
      "read file ..  100058.txt\n",
      "read file ..  100059.txt\n",
      "read file ..  10006.txt\n",
      "read file ..  100060.txt\n",
      "read file ..  100061.txt\n",
      "read file ..  100062.txt\n",
      "read file ..  100063.txt\n",
      "read file ..  100064.txt\n",
      "read file ..  100065.txt\n",
      "read file ..  100066.txt\n",
      "read file ..  100067.txt\n",
      "read file ..  100068.txt\n",
      "read file ..  100069.txt\n",
      "read file ..  10007.txt\n",
      "read file ..  100070.txt\n",
      "read file ..  100071.txt\n",
      "read file ..  100072.txt\n",
      "read file ..  100073.txt\n",
      "read file ..  100074.txt\n",
      "read file ..  100075.txt\n",
      "read file ..  100076.txt\n",
      "read file ..  100077.txt\n",
      "read file ..  100078.txt\n",
      "read file ..  100079.txt\n",
      "read file ..  10008.txt\n",
      "read file ..  100080.txt\n",
      "read file ..  100081.txt\n",
      "read file ..  100082.txt\n",
      "read file ..  100083.txt\n",
      "read file ..  100084.txt\n",
      "read file ..  100085.txt\n",
      "read file ..  100086.txt\n",
      "read file ..  100087.txt\n",
      "read file ..  100088.txt\n",
      "read file ..  100089.txt\n",
      "read file ..  10009.txt\n",
      "read file ..  100090.txt\n",
      "read file ..  100091.txt\n",
      "read file ..  100092.txt\n",
      "read file ..  100093.txt\n",
      "read file ..  100094.txt\n",
      "read file ..  100095.txt\n",
      "read file ..  100096.txt\n",
      "read file ..  100097.txt\n",
      "read file ..  100098.txt\n",
      "read file ..  100099.txt\n",
      "read file ..  1001.txt\n",
      "read file ..  10010.txt\n",
      "read file ..  100100.txt\n",
      "read file ..  100101.txt\n",
      "read file ..  100102.txt\n",
      "read file ..  100103.txt\n",
      "read file ..  100104.txt\n",
      "read file ..  100105.txt\n",
      "read file ..  100106.txt\n",
      "read file ..  100107.txt\n",
      "read file ..  100108.txt\n",
      "read file ..  100109.txt\n",
      "read file ..  10011.txt\n",
      "read file ..  100110.txt\n",
      "read file ..  100111.txt\n",
      "read file ..  100112.txt\n",
      "read file ..  100113.txt\n",
      "read file ..  100114.txt\n",
      "read file ..  100115.txt\n",
      "read file ..  100116.txt\n",
      "read file ..  100117.txt\n",
      "read file ..  100118.txt\n",
      "read file ..  100119.txt\n",
      "read file ..  10012.txt\n",
      "read file ..  100120.txt\n",
      "read file ..  100121.txt\n",
      "read file ..  100122.txt\n",
      "read file ..  100123.txt\n",
      "read file ..  100124.txt\n",
      "read file ..  100125.txt\n",
      "read file ..  100126.txt\n",
      "read file ..  100127.txt\n",
      "read file ..  100128.txt\n",
      "read file ..  100129.txt\n",
      "read file ..  10013.txt\n",
      "read file ..  100130.txt\n",
      "read file ..  100131.txt\n",
      "read file ..  100132.txt\n",
      "read file ..  100133.txt\n",
      "read file ..  100134.txt\n",
      "read file ..  100135.txt\n",
      "read file ..  100136.txt\n",
      "read file ..  100137.txt\n",
      "read file ..  100138.txt\n",
      "read file ..  100139.txt\n",
      "read file ..  10014.txt\n",
      "read file ..  100140.txt\n",
      "read file ..  100141.txt\n",
      "read file ..  100142.txt\n",
      "read file ..  100143.txt\n",
      "read file ..  100144.txt\n",
      "read file ..  100145.txt\n",
      "read file ..  100146.txt\n",
      "read file ..  100147.txt\n",
      "read file ..  100148.txt\n",
      "read file ..  100149.txt\n",
      "read file ..  10015.txt\n",
      "read file ..  100150.txt\n",
      "read file ..  100151.txt\n",
      "read file ..  100152.txt\n",
      "read file ..  100153.txt\n",
      "read file ..  100154.txt\n",
      "read file ..  100155.txt\n",
      "read file ..  100156.txt\n",
      "read file ..  100157.txt\n",
      "read file ..  100158.txt\n",
      "read file ..  100159.txt\n",
      "read file ..  10016.txt\n",
      "read file ..  100160.txt\n",
      "read file ..  100161.txt\n",
      "read file ..  100162.txt\n",
      "read file ..  100163.txt\n",
      "read file ..  100164.txt\n",
      "read file ..  100165.txt\n",
      "read file ..  100166.txt\n",
      "read file ..  100167.txt\n",
      "read file ..  100168.txt\n",
      "read file ..  100169.txt\n",
      "read file ..  10017.txt\n",
      "read file ..  100170.txt\n",
      "read file ..  100171.txt\n",
      "read file ..  100172.txt\n",
      "read file ..  100173.txt\n",
      "read file ..  100174.txt\n",
      "read file ..  100175.txt\n",
      "read file ..  100176.txt\n",
      "read file ..  100177.txt\n",
      "read file ..  100178.txt\n",
      "read file ..  100179.txt\n",
      "read file ..  10018.txt\n",
      "read file ..  100180.txt\n",
      "read file ..  100181.txt\n",
      "read file ..  100182.txt\n",
      "read file ..  100183.txt\n",
      "read file ..  100184.txt\n",
      "read file ..  100185.txt\n",
      "read file ..  100186.txt\n",
      "read file ..  100187.txt\n",
      "read file ..  100188.txt\n",
      "read file ..  100189.txt\n",
      "read file ..  10019.txt\n",
      "read file ..  100190.txt\n",
      "read file ..  100191.txt\n",
      "read file ..  100192.txt\n",
      "read file ..  100193.txt\n",
      "read file ..  100194.txt\n",
      "read file ..  100195.txt\n",
      "read file ..  100196.txt\n",
      "read file ..  100197.txt\n",
      "read file ..  100198.txt\n",
      "read file ..  100199.txt\n",
      "read file ..  1002.txt\n",
      "read file ..  10020.txt\n",
      "read file ..  100200.txt\n",
      "read file ..  100201.txt\n",
      "read file ..  100202.txt\n",
      "read file ..  100203.txt\n",
      "read file ..  100204.txt\n",
      "read file ..  100205.txt\n",
      "read file ..  100206.txt\n",
      "read file ..  100207.txt\n",
      "read file ..  100208.txt\n",
      "read file ..  100209.txt\n",
      "read file ..  10021.txt\n",
      "read file ..  100210.txt\n",
      "read file ..  100211.txt\n",
      "read file ..  100212.txt\n",
      "read file ..  100213.txt\n",
      "read file ..  100214.txt\n",
      "read file ..  100215.txt\n",
      "read file ..  100216.txt\n",
      "read file ..  100217.txt\n",
      "read file ..  100218.txt\n",
      "read file ..  100219.txt\n",
      "read file ..  10022.txt\n",
      "read file ..  100220.txt\n",
      "read file ..  100221.txt\n",
      "read file ..  100222.txt\n",
      "read file ..  100223.txt\n",
      "read file ..  100224.txt\n",
      "read file ..  100225.txt\n",
      "read file ..  100226.txt\n",
      "read file ..  100227.txt\n",
      "read file ..  100228.txt\n",
      "read file ..  100229.txt\n",
      "read file ..  10023.txt\n",
      "read file ..  100230.txt\n",
      "read file ..  100231.txt\n",
      "read file ..  100232.txt\n",
      "read file ..  100233.txt\n",
      "read file ..  100234.txt\n",
      "read file ..  100235.txt\n",
      "read file ..  100236.txt\n",
      "read file ..  100237.txt\n",
      "read file ..  100238.txt\n",
      "read file ..  100239.txt\n",
      "read file ..  10024.txt\n",
      "read file ..  100240.txt\n",
      "read file ..  100241.txt\n",
      "read file ..  100242.txt\n",
      "read file ..  100243.txt\n",
      "read file ..  100244.txt\n",
      "read file ..  100245.txt\n",
      "read file ..  100246.txt\n",
      "read file ..  100247.txt\n",
      "read file ..  100248.txt\n",
      "read file ..  100249.txt\n",
      "read file ..  10025.txt\n",
      "read file ..  100250.txt\n",
      "read file ..  100251.txt\n",
      "read file ..  100252.txt\n",
      "read file ..  100253.txt\n",
      "read file ..  100254.txt\n",
      "read file ..  100255.txt\n",
      "read file ..  100256.txt\n",
      "read file ..  100257.txt\n",
      "read file ..  100258.txt\n",
      "read file ..  100259.txt\n",
      "read file ..  10026.txt\n",
      "read file ..  100260.txt\n",
      "read file ..  100261.txt\n",
      "read file ..  100262.txt\n",
      "read file ..  100263.txt\n",
      "read file ..  100264.txt\n",
      "read file ..  100265.txt\n",
      "read file ..  100266.txt\n",
      "read file ..  100267.txt\n",
      "read file ..  100268.txt\n",
      "read file ..  100269.txt\n",
      "read file ..  10027.txt\n",
      "read file ..  100270.txt\n",
      "read file ..  100271.txt\n",
      "read file ..  100272.txt\n",
      "read file ..  100273.txt\n",
      "read file ..  100274.txt\n",
      "read file ..  100275.txt\n",
      "read file ..  100276.txt\n",
      "read file ..  100277.txt\n",
      "read file ..  100278.txt\n",
      "read file ..  100279.txt\n",
      "read file ..  10028.txt\n",
      "read file ..  100280.txt\n",
      "read file ..  100281.txt\n",
      "read file ..  100282.txt\n",
      "read file ..  100283.txt\n",
      "read file ..  100284.txt\n",
      "read file ..  100285.txt\n",
      "read file ..  100286.txt\n",
      "read file ..  100287.txt\n",
      "read file ..  100288.txt\n",
      "read file ..  100289.txt\n",
      "read file ..  10029.txt\n",
      "read file ..  100290.txt\n",
      "read file ..  100291.txt\n",
      "read file ..  100292.txt\n",
      "read file ..  100293.txt\n",
      "read file ..  100294.txt\n",
      "read file ..  100295.txt\n",
      "read file ..  100296.txt\n",
      "read file ..  100297.txt\n",
      "read file ..  100298.txt\n",
      "read file ..  100299.txt\n",
      "read file ..  1003.txt\n",
      "read file ..  10030.txt\n",
      "read file ..  100300.txt\n",
      "read file ..  100301.txt\n",
      "read file ..  100302.txt\n",
      "read file ..  100303.txt\n",
      "read file ..  100304.txt\n",
      "read file ..  100305.txt\n",
      "read file ..  100306.txt\n",
      "read file ..  100307.txt\n",
      "read file ..  100308.txt\n",
      "read file ..  100309.txt\n",
      "read file ..  10031.txt\n",
      "read file ..  100310.txt\n",
      "read file ..  100311.txt\n",
      "read file ..  100312.txt\n",
      "read file ..  100313.txt\n",
      "read file ..  100314.txt\n",
      "read file ..  100315.txt\n",
      "read file ..  100316.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file ..  100317.txt\n",
      "read file ..  100318.txt\n",
      "read file ..  100319.txt\n",
      "read file ..  10032.txt\n",
      "read file ..  100320.txt\n",
      "read file ..  100321.txt\n",
      "read file ..  100322.txt\n",
      "read file ..  100323.txt\n",
      "read file ..  100324.txt\n",
      "read file ..  100325.txt\n",
      "read file ..  100326.txt\n",
      "read file ..  100327.txt\n",
      "read file ..  100328.txt\n",
      "read file ..  100329.txt\n",
      "read file ..  10033.txt\n",
      "read file ..  100330.txt\n",
      "read file ..  100331.txt\n",
      "read file ..  100332.txt\n",
      "read file ..  100333.txt\n",
      "read file ..  100334.txt\n",
      "read file ..  100335.txt\n",
      "read file ..  100336.txt\n",
      "read file ..  100337.txt\n",
      "read file ..  100338.txt\n",
      "read file ..  100339.txt\n",
      "read file ..  10034.txt\n",
      "read file ..  100340.txt\n",
      "read file ..  100341.txt\n",
      "read file ..  100342.txt\n",
      "read file ..  100343.txt\n",
      "read file ..  100344.txt\n",
      "read file ..  100345.txt\n",
      "read file ..  100346.txt\n",
      "read file ..  100347.txt\n",
      "read file ..  100348.txt\n",
      "read file ..  100349.txt\n",
      "read file ..  10035.txt\n",
      "read file ..  100350.txt\n",
      "read file ..  100351.txt\n",
      "read file ..  100352.txt\n",
      "read file ..  100353.txt\n",
      "read file ..  100354.txt\n",
      "read file ..  100355.txt\n",
      "read file ..  100356.txt\n",
      "read file ..  100357.txt\n",
      "read file ..  100358.txt\n",
      "read file ..  100359.txt\n",
      "read file ..  10036.txt\n",
      "read file ..  100360.txt\n",
      "read file ..  100361.txt\n",
      "read file ..  100362.txt\n",
      "read file ..  100363.txt\n",
      "read file ..  100364.txt\n",
      "read file ..  100365.txt\n",
      "read file ..  100366.txt\n",
      "read file ..  100367.txt\n",
      "read file ..  100368.txt\n",
      "read file ..  100369.txt\n",
      "read file ..  10037.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12532/2315892069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meachFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mfileName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meachFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mopenfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0meachFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this part of the code reads the folder and creates the dataframes\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"lyrics_files_idioms/\"\n",
    "\n",
    "# print(onlyfiles)\n",
    "\n",
    "fileName = []\n",
    "language = []\n",
    "link = []\n",
    "title = []\n",
    "lyric = []\n",
    "\n",
    "for eachFile in listdir(mypath):\n",
    "    if isfile(join(mypath, eachFile)):\n",
    "        fileName.append(eachFile.split(\".\")[0])\n",
    "        openfile = open(mypath+\"/\"+eachFile, 'r', encoding='utf-8')\n",
    "        lines = openfile.readlines()\n",
    "\n",
    "        language.append(lines[0])\n",
    "        link.append(lines[1].replace(\"\\\\n\",\"\"))\n",
    "        title.append(lines[2].replace(\"\\\\n\",\"\"))\n",
    "        lyric.append(lines[3].replace(\"\\\\n\",\"\"))\n",
    "        print(\"read file .. \", eachFile)\n",
    "        openfile.close()\n",
    "\n",
    "print(len(fileName))    \n",
    "print(len(language))\n",
    "print(len(link))\n",
    "print(len(title))\n",
    "print(len(lyric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>language</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ENGLISH\\n</td>\n",
       "      <td>/10000-maniacs/more-than-this.html\\n</td>\n",
       "      <td>More Than This\\n</td>\n",
       "      <td>I could feel at the time. There was no way of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>ENGLISH\\n</td>\n",
       "      <td>/10000-maniacs/anthem-for-doomed-youth.html\\n</td>\n",
       "      <td>Anthem For Doomed Youth\\n</td>\n",
       "      <td>For whom do the bells toll. When sentenced to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>ENGLISH\\n</td>\n",
       "      <td>/10000-maniacs/planned-obsolescence.html\\n</td>\n",
       "      <td>Planned Obsolescence\\n</td>\n",
       "      <td>[ music: Dennis Drew/lyric: Natalie Merchant ]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>ENGLISH\\n</td>\n",
       "      <td>/ac-dc/night-of-the-long-knives.html\\n</td>\n",
       "      <td>Night Of The Long Knives\\n</td>\n",
       "      <td>Who's your leader, who's your man?. Who will h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>ENGLISH\\n</td>\n",
       "      <td>/buffalo-springfield/bluebird.html\\n</td>\n",
       "      <td>Bluebird\\n</td>\n",
       "      <td>Listen to my bluebird laugh.. She can't tell y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fileName   language                                           link  \\\n",
       "0         1  ENGLISH\\n           /10000-maniacs/more-than-this.html\\n   \n",
       "1        10  ENGLISH\\n  /10000-maniacs/anthem-for-doomed-youth.html\\n   \n",
       "2       100  ENGLISH\\n     /10000-maniacs/planned-obsolescence.html\\n   \n",
       "3      1000  ENGLISH\\n         /ac-dc/night-of-the-long-knives.html\\n   \n",
       "4     10000  ENGLISH\\n           /buffalo-springfield/bluebird.html\\n   \n",
       "\n",
       "                        title  \\\n",
       "0            More Than This\\n   \n",
       "1   Anthem For Doomed Youth\\n   \n",
       "2      Planned Obsolescence\\n   \n",
       "3  Night Of The Long Knives\\n   \n",
       "4                  Bluebird\\n   \n",
       "\n",
       "                                               lyric  \n",
       "0  I could feel at the time. There was no way of ...  \n",
       "1  For whom do the bells toll. When sentenced to ...  \n",
       "2  [ music: Dennis Drew/lyric: Natalie Merchant ]...  \n",
       "3  Who's your leader, who's your man?. Who will h...  \n",
       "4  Listen to my bluebird laugh.. She can't tell y...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(list(zip(fileName, language, link, title, lyric)), \n",
    "#                   columns =[\"fileName\", \"language\", \"link\", \"title\", \"lyric\"] )\n",
    "\n",
    "# saving memory\n",
    "# fileName.clear()\n",
    "# language.clear()\n",
    "# link.clear()\n",
    "# title.clear()\n",
    "# lyric.clear()\n",
    "# df.head()\n",
    "\n",
    "df = pd.read_csv('./lyrics_all_files.csv')\n",
    "df = df[[\"fileName\", \"language\", \"link\", \"title\", \"lyric\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkdf = spark.createDataFrame(df)\n",
    "sparkrdd = sparkdf.rdd\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sparkrdd.take(5))\n",
    "link_count = sparkrdd.map(lambda perlink : (perlink[2],1)).reduceByKey(lambda a,b : a+b).filter(lambda b : b[1]>=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/10000-maniacs/a-campfire-song.html\\n', 2), ('/10000-maniacs/a-room-for-everything.html\\n', 2), ('/10000-maniacs/across-the-fields.html\\n', 2), ('/10000-maniacs/all-that-never-happens.html\\n', 2), ('/10000-maniacs/anthem-for-doomed-youth.html\\n', 2)]\n",
      "38096\n"
     ]
    }
   ],
   "source": [
    "#songs lyrics/url appearing atleast 2 times.\n",
    "print(link_count.sortByKey().take(5))\n",
    "print(link_count.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK49MFw9M8yA"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "### 2.1 - (1 point) - Distinct songs\n",
    "Provide a Spark MapReduce procedure that provides how many distinct song's lyrics are present.\n",
    "\n",
    "Also in this case consider the uri as key: two files represent the same lyric if the url is equal.\n",
    "\n",
    "### 2.2 - (1 point) - Chaining\n",
    "According to your implementation of Exercise 1, can you chain MapReduce additional MapReduce steps for solving Exercise 2.2? \n",
    "\n",
    "Provide the code for 2.1 and anwer for 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/021-funcao/a-cura.html\\n', 1), ('/021-funcao/circo-de-horrores.html\\n', 1), ('/021-funcao/leve-poesia-me-traga-sorte.html\\n', 1), ('/021-funcao/noite-anterior.html\\n', 1), ('/021-funcao/quer-fritar-acompanha.html\\n', 1)]\n",
      "167499\n",
      "[(1, '/abba/al-andar.html\\n'), (1, '/abba/as-good-as-new.html\\n'), (1, '/abba/att-finnas-till.html\\n'), (1, '/abba/autumn-days-hamlet-i-ii.html\\n'), (1, '/abba/baby-cifrada.html\\n')]\n",
      "167499\n"
     ]
    }
   ],
   "source": [
    "### Write here your code followed by the answer to question 2.2\n",
    "\n",
    "#2.1\n",
    "#distinct song's lyrics\n",
    "link_count = sparkrdd.map(lambda perlink : (perlink[2],1)).reduceByKey(lambda a,b : a+b).filter(lambda b : b[1]>=1)\n",
    "print(link_count.sortByKey().take(5))\n",
    "print(link_count.count())\n",
    "\n",
    "#2.2 \n",
    "#chaining mapreduce\n",
    "chain_link_mr1 = sparkrdd.map(lambda perlink : (perlink[2],1)).reduceByKey(lambda a,b : a+b)\n",
    "chain_link_mr2 = chain_link_mr1.filter(lambda b : b[1]>=1).map(lambda b :  (b[1],b[0])).sortByKey()\n",
    "\n",
    "print(chain_link_mr2.take(5))\n",
    "print(chain_link_mr2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "### 3.1 - (3 points) - Most common word for language\n",
    "\n",
    "Now that you discovered the duplicated documents consider just one occurence of each song's lyric and define a MapReduce procedure that finds the most common word for each language (of course you must remove stop words).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_stopwords(lang):\n",
    "    try:\n",
    "        return stopwords.words(lang)\n",
    "    except:\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['How', 'new', 'year', 'been,', 'go', '?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(get_stopwords('hindi'))\n",
    "print(get_stopwords('english'))\n",
    "\n",
    "[word for word in \"How has your new year been, did you go out ?\".split(\" \") if not word in get_stopwords(\"English\".lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ENGLISH', Counter({'the': 16, 'of': 5, 'to': 4, 'will': 4, 'their': 4, 'a': 3, 'like': 3, 'and': 3, 'or': 3, 'for': 2, 'bells': 2, 'when': 2, 'me': 2, \"he's\": 2, 'can': 2, 'be': 2, 'no': 2, 'them': 2, 'wailing': 2, 'on': 2, \"it's\": 2, 'our': 2, 'what': 2, 'whom': 1, 'do': 1, 'toll': 1, 'sentenced': 1, 'die': 1, 'stuttering': 1, 'rifles': 1, 'stifle': 1, 'cry': 1, 'monstrous': 1, 'anger': 1, \"fear's\": 1, 'rapid': 1, 'rattle': 1, 'desert': 1, 'inferno': 1, 'kids': 1, 'dying': 1, 'cattle': 1, \"don't\": 1, 'tell': 1, \"we're\": 1, 'not': 1, 'prepared': 1, \"i've\": 1, 'seen': 1, \"today's\": 1})), ('SPANISH', Counter({'el': 24, 'y': 19, 'al': 14, 'andar': 14, 'la': 13, 'como': 12, 'que': 9, 'va': 8, 'lo': 7, 'su': 7, 'es': 5, 'sol': 5, 'momento': 5, 'siento': 5, 'mar': 4, 'viento': 4, 'corrida': 4, 'vida': 4, 'a': 4, 'mañana': 4, 'reclama': 4, 'tesoro': 4, 'vivo': 4, 'mi': 4, 'intento': 4, 'en': 3, 'si': 3, 'creciente': 3, 'gente': 3, 'hago': 3, 'o': 2, 'mucho': 2, 'no': 2, 'sé': 2, 'con': 2, 'sin': 2, 'quien': 2, 'las': 2, 'vivir': 2, 'se': 1, 'dicen': 1, 'ansioso': 1, 'alma': 1, 'paz': 1, 'viajero': 1, 'misionero': 1, 'lejos': 1, 'más': 1, 'miro': 1, 'hasta': 1})), ('ENGLISH', Counter({'ma': 32, 'as': 18, 'i': 14, 'new': 11, 'my': 10, 'to': 9, 'you': 9, 'good': 9, 'and': 7, 'love': 7, 'a': 6, 'that': 6, \"it's\": 6, 'for': 5, 'had': 4, 'here': 4, 'is': 4, 'it': 4, 'never': 3, 'yes': 3, 'like': 3, 'be': 3, 'darling': 3, 'we': 3, 'were': 3, 'always': 3, 'meant': 3, 'stay': 3, 'together': 3, 'know': 2, 'why': 2, 'such': 2, 'was': 2, 'now': 2, 'at': 2, 'again': 2, \"'cause\": 2, 'found': 2, 'out': 2, 'life': 2, 'gotta': 2, 'have': 2, 'near': 2, 'keeping': 2, 'way': 2, 'intention': 2, 'growing': 2, 'too': 2, 'think': 2, 'taking': 2})), ('SWEDISH', Counter({'att': 20, 'är': 15, 'det': 14, 'till': 14, 'finnas': 11, '': 10, 'ett': 5, 'för': 4, 'man': 4, 'en': 3, 'mig': 3, 'se': 3, 'då': 3, 'jag': 3, 'med': 3, 'leva': 2, 'i': 2, 'från': 2, 'ligga': 2, 'känner': 2, 'finns': 2, 'njuta': 2, 'stunder': 2, 'så': 2, 'stort': 2, 'herrens': 2, 'under': 2, \"va'\": 2, 'dig': 2, 'den': 2, 'kanske': 2, 'vad': 1, 'livet': 1, 'nöjen': 1, 'stor': 1, 'stad': 1, 'eller': 1, 'tystnad': 1, 'på': 1, 'ö': 1, 'sommarängar': 1, 'blomma': 1, 'lyss': 1, 'vågor': 1, 'sjö': 1, 'hav': 1, 'solnedgången': 1, 'segel': 1, 'still': 1, 'sommarkväll': 1})), ('ENGLISH', Counter({'the': 13, 'i': 8, 'of': 6, 'you': 6, 'on': 6, 'and': 5, 'autumn': 4, 'memories': 4, 'were': 3, 'rain': 3, 'fell': 3, 'softly': 3, 'your': 3, 'face': 3, 'our': 3, 'time': 3, 'together': 3, 'cannot': 3, 'be': 3, 'replaced': 3, 'never': 3, 'loved': 3, 'more': 3, 'than': 3, 'those': 3, 'happy': 3, 'days': 3, 'when': 2, 'it': 2, 'my': 2, 'remember': 2, 'every': 2, 'grey': 2, 'skies': 2, 'laughing': 2, 'at': 2, 'clouds': 2, 'leaves': 1, 'are': 1, 'falling': 1, 'to': 1, 'ground': 1, 'air': 1, 'gets': 1, 'cold': 1, 'then': 1, 'think': 1, 'us': 1, 'almost': 1, 'makes': 1}))]\n"
     ]
    }
   ],
   "source": [
    "#one occurrence of each song\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "link_count = sparkrdd \\\n",
    ".map(lambda perlink : (perlink[2], (perlink[1],perlink[4]))) \\\n",
    ".reduceByKey(lambda a,b : b) \\\n",
    ".map(lambda x : x[1] ) \\\n",
    ".map(lambda x : (x[0].strip(), x[1].strip())) \\\n",
    ".filter(lambda x: x[0].lower() != \"na\") \\\n",
    ".map( lambda x : (x[0].replace(\"\\n\",\"\"), Counter(re.sub(r\"[.,\\(\\)?!_\\-\\d]+\", '', x[1].lower()).split(\" \")) )) \\\n",
    ".map(lambda x : (x[0], Counter(dict((x[1]).most_common(50)))))\n",
    "\n",
    "\n",
    "# .map( lambda x : (x[0], [word for word in x[1] if not word in get_stopwords(x[0].lower())] )) \\\n",
    "print(link_count.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENGLISH',\n",
       "  [('the', 920875),\n",
       "   ('you', 879073),\n",
       "   ('i', 876891),\n",
       "   ('to', 529315),\n",
       "   ('and', 521405)]),\n",
       " ('SPANISH',\n",
       "  [('que', 35778), ('y', 24085), ('no', 23108), ('de', 20780), ('la', 17805)]),\n",
       " ('SWEDISH',\n",
       "  [('jag', 230), ('det', 176), ('är', 170), ('och', 148), ('du', 120)]),\n",
       " ('FRENCH',\n",
       "  [('je', 2451), ('de', 2183), ('la', 1592), ('et', 1568), ('le', 1472)]),\n",
       " ('GALICIAN', [('hai', 63), ('a', 42), ('paso', 36), ('de', 34), ('', 33)]),\n",
       " ('ESTONIAN',\n",
       "  [('sa', 41), ('ja', 26), ('jää', 20), ('kuldseks', 19), ('ma', 18)]),\n",
       " ('CATALAN',\n",
       "  [('la', 34), ('tu', 29), ('barcelona', 24), ('que', 23), ('de', 22)]),\n",
       " ('CEBUANO', [('tibum', 32), ('pararã¡', 30), ('parará', 1), ('parar�¡', 1)]),\n",
       " ('CROATIAN', [('i', 38), ('u', 23), ('da', 20), ('je', 13), ('koji', 11)]),\n",
       " ('AFRIKAANS', [('ons', 7), ('die', 5), ('kom', 4), ('met', 4), ('my', 4)]),\n",
       " ('SLOVAK',\n",
       "  [('uncoolohol', 16), ('the', 10), ('of', 8), ('and', 5), ('war', 3)]),\n",
       " ('ROMANIAN',\n",
       "  [(\"pe'\", 3), ('te', 3), ('cu', 3), ('tenímmoce', 2), ('anema', 2)]),\n",
       " ('BOSNIAN', [('je', 8), ('i', 7), ('u', 6), ('kad', 6), ('jedan', 5)]),\n",
       " ('PORTUGUESE',\n",
       "  [('que', 377673),\n",
       "   ('o', 304297),\n",
       "   ('eu', 291306),\n",
       "   ('e', 271840),\n",
       "   ('a', 268176)]),\n",
       " ('POLISH', [('czy', 11), ('nie', 6), ('policz', 6), ('do', 6), ('siê', 5)]),\n",
       " ('HAITIAN_CREOLE',\n",
       "  [('nou', 83), ('yo', 83), ('pa', 77), ('mwen', 58), ('se', 54)]),\n",
       " ('KOREAN', [('you', 16), ('재', 12), ('재수없어', 9), ('없어', 9), ('hate', 8)]),\n",
       " ('GANDA',\n",
       "  [('eh', 51), ('nal', 48), ('ddara', 48), ('haebwayo', 48), ('ireoke', 16)]),\n",
       " ('HMONG', [('me', 32), ('sento', 32), ('na', 32), ('rua', 32)]),\n",
       " ('HUNGARIAN',\n",
       "  [('virágom', 4), ('tavaszi', 1), ('szél', 1), ('vizet', 1), ('áraszt', 1)]),\n",
       " ('MALAY', [('the', 9), ('di', 5), ('of', 5), ('sing', 4), ('children', 4)]),\n",
       " ('SUNDANESE',\n",
       "  [('you', 34),\n",
       "   ('i', 27),\n",
       "   ('on', 20),\n",
       "   ('nanananananananananananana', 16),\n",
       "   ('the', 16)]),\n",
       " ('KURDISH',\n",
       "  [('kevin', 12), ('carter', 12), ('click', 6), ('to', 5), (\"'alayna\", 4)]),\n",
       " ('INDONESIAN',\n",
       "  [('i', 38), ('your', 36), ('get', 33), ('yer', 32), ('nah', 32)]),\n",
       " ('GERMAN',\n",
       "  [('ich', 1675), ('und', 1369), ('die', 1154), ('du', 1051), ('nicht', 858)]),\n",
       " ('KINYARWANDA',\n",
       "  [('ni', 712), ('wo', 544), ('no', 543), ('wa', 344), ('ga', 215)]),\n",
       " ('JAPANESE',\n",
       "  [('huh', 66), ('la', 63), ('rock', 18), ('your', 13), ('gear', 13)]),\n",
       " ('IRISH', [('me', 67), ('the', 54), ('let', 39), ('a', 37), ('go', 36)]),\n",
       " ('FINNISH', [('on', 74), ('the', 34), ('ja', 31), ('kun', 16), ('nyt', 13)]),\n",
       " ('ICELANDIC', [('og', 125), ('í', 101), ('á', 79), ('ég', 75), ('við', 73)]),\n",
       " ('SESOTHO',\n",
       "  [('eh', 51), ('na', 36), ('hae', 36), ('bayo', 30), ('sarang', 18)]),\n",
       " ('DANISH',\n",
       "  [('forever', 121), ('i', 66), ('you', 39), ('feel', 36), ('love', 36)]),\n",
       " ('RUSSIAN',\n",
       "  [('стиле', 25), ('на', 25), ('и', 25), ('мы', 21), ('забери', 20)]),\n",
       " ('BASQUE', [('ez', 24), ('eta', 19), ('ni', 16), ('nire', 12), ('naiz', 11)]),\n",
       " ('SERBIAN', [('i', 13), ('nego', 8), ('o', 6), ('u', 5), ('se', 5)]),\n",
       " ('NYANJA', [('machika', 69), ('no', 13), ('más', 9), ('y', 9), ('la', 8)]),\n",
       " ('SLOVENIAN',\n",
       "  [('nekej', 9), ('kot', 6), ('je', 5), ('pozitiv', 5), ('vabrejšan', 5)]),\n",
       " ('ARABIC', [('y', 21), ('yo', 15), (\"lejo'\", 15), ('me', 14), ('nos', 11)]),\n",
       " ('TAGALOG',\n",
       "  [('ang', 79), ('bebot', 72), ('chika', 61), ('sa', 44), ('na', 41)]),\n",
       " ('ITALIAN',\n",
       "  [('che', 3858), ('e', 3840), ('non', 3102), ('di', 2217), ('a', 1656)]),\n",
       " ('MALAGASY',\n",
       "  [('me', 38), ('que', 36), ('tanto', 30), ('oe', 28), ('gustas', 27)]),\n",
       " ('CZECH',\n",
       "  [('senta', 13), ('leva', 12), ('pro', 12), ('mirante', 12), ('tu', 4)]),\n",
       " ('SWAHILI',\n",
       "  [('pouss', 56), ('ah', 28), (\"b'wakawa\", 28), ('i', 22), ('a', 13)]),\n",
       " ('NORWEGIAN',\n",
       "  [('vestido', 32), ('estampado', 32), ('i', 22), ('jeg', 19), ('han', 15)]),\n",
       " ('DUTCH', [('ik', 44), ('nimma', 40), ('je', 40), ('een', 35), ('en', 29)]),\n",
       " ('WELSH', [('sally', 8), ('mae', 8), ('i', 6), ('a', 5), ('do', 5)]),\n",
       " ('TURKISH', [('loco', 19), ('i', 12), ('you', 11), ('me', 10), ('got', 9)])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_count = link_count \\\n",
    ".reduceByKey(lambda a,b : Counter(dict((a + b).most_common(50)))) \\\n",
    ".map(lambda x : (x[0], x[1].most_common(5)))\n",
    "\n",
    "link_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note -> NLTK library dont work in my laptop with mapreduce and hence the code fails everytime I add removing stopwords part\n",
    "\n",
    "without_stopwords = sparkrdd \\\n",
    ".map(lambda perlink : (perlink[2], (perlink[1],perlink[4]))) \\\n",
    ".reduceByKey(lambda a,b : b) \\\n",
    ".map(lambda x : x[1] ) \\\n",
    ".map(lambda x : (x[0].strip(), x[1].strip())) \\\n",
    ".filter(lambda x: x[0].lower() != \"na\") \\\n",
    ".map( lambda x : (x[0].replace(\"\\n\",\"\"), re.sub(r\"[.,\\(\\)?!_\\-\\d]+\", '', x[1].lower()).split(\" \")) ) \\\n",
    ".map( lambda x : (x[0], [word for word in x[1] if not word in get_stopwords(x[0].lower())] )) \\\n",
    ".map(lambda x : (x[0], Counter(dict((Counter(x[1])).most_common(50))))) \\\n",
    ".reduceByKey(lambda a,b : Counter(dict((a + b).most_common(50)))) \\\n",
    ".map(lambda x : (x[0], x[1].most_common(1)))\n",
    "\n",
    "without_stopwords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - (3 points) - Most common end/start words\n",
    "\n",
    "Finally discover, for each language, the most common ending and starting word (of course, also in this case) you must remove stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENGLISH',\n",
       "  [('i', 9754), ('you', 3101), (\"i'm\", 2131), ('when', 1465), ('the', 1438)]),\n",
       " ('SPANISH', [('no', 162), ('yo', 110), ('hoy', 69), ('ya', 63), ('me', 62)]),\n",
       " ('SWEDISH', [('jag', 2), ('från', 2), ('vad', 1), ('för', 1), ('varfor', 1)]),\n",
       " ('FRENCH',\n",
       "  [('je', 42), (\"j'ai\", 13), (\"c'est\", 11), ('dans', 10), ('il', 10)]),\n",
       " ('GALICIAN',\n",
       "  [('a', 1), ('ruas', 1), ('¿que', 1), ('claridade', 1), ('los', 1)]),\n",
       " ('ESTONIAN',\n",
       "  [('elus', 2), ('suda', 1), ('[walking', 1), (\"i'm\", 1), ('me', 1)]),\n",
       " ('CATALAN', [('rambla', 1), ('bendito', 1), ('oh', 1), ('si', 1), ('en', 1)]),\n",
       " ('CEBUANO', [('parará', 1)]),\n",
       " ('CROATIAN', [('u', 1), ('krv', 1), ('o', 1), ('ja', 1), ('oštrim', 1)]),\n",
       " ('AFRIKAANS', [('kom', 1)]),\n",
       " ('SLOVAK', [('the', 1)]),\n",
       " ('ROMANIAN', [('tenímmoce', 1)]),\n",
       " ('BOSNIAN', [('oko', 1)]),\n",
       " ('PORTUGUESE',\n",
       "  [('eu', 5567), ('o', 2167), ('não', 2138), ('a', 1657), ('se', 1286)]),\n",
       " ('POLISH', [('nie', 1)]),\n",
       " ('HAITIAN_CREOLE', [('men', 2), ('[tropical', 1), ('haiti', 1), ('feat', 1)]),\n",
       " ('KOREAN', [('넌', 1)]),\n",
       " ('GANDA', [('ne', 1)]),\n",
       " ('HMONG', [('me', 1)]),\n",
       " ('HUNGARIAN', [('tavaszi', 1)]),\n",
       " ('MALAY', [('jangankan', 1), ('selamanya', 1), ('oh', 1), ('dua', 1)]),\n",
       " ('SUNDANESE',\n",
       "  [('uyeonhi', 1),\n",
       "   ('mayakovsky', 1),\n",
       "   ('apeudorok', 1),\n",
       "   ('eongdeongi', 1),\n",
       "   ('mentareun', 1)]),\n",
       " ('KURDISH', [(\"tala'albadru\", 1), ('hi', 1)]),\n",
       " ('INDONESIAN',\n",
       "  [(\"i'm\", 1), (\"you're\", 1), ('ya', 1), ('hey', 1), ('lihatlah', 1)]),\n",
       " ('GERMAN', [('ich', 36), ('du', 18), ('wir', 10), ('die', 9), ('es', 9)]),\n",
       " ('KINYARWANDA',\n",
       "  [('romanji', 3), ('sora', 3), ('kodomo', 2), ('sousa', 2), ('ansoku', 2)]),\n",
       " ('JAPANESE', [('say', 1), ('rock', 1)]),\n",
       " ('IRISH',\n",
       "  [('is', 1), ('go', 1), ('buachaill', 1), ('an', 1), ('tumbalacatumba', 1)]),\n",
       " ('FINNISH',\n",
       "  [('en', 2), ('kun', 2), ('nukkuu', 1), ('pakkasyössä', 1), ('kerran', 1)]),\n",
       " ('ICELANDIC',\n",
       "  [('ég', 4), ('þú', 3), ('á', 2), ('fjöll', 1), ('hamagangur', 1)]),\n",
       " ('SESOTHO', [('we', 1), ('ne', 1)]),\n",
       " ('DANISH',\n",
       "  [('do', 2), ('forever', 2), ('harder', 1), (\"i'm\", 1), ('people', 1)]),\n",
       " ('RUSSIAN', [('муз', 2), ('а', 1), ('забери', 1)]),\n",
       " ('BASQUE',\n",
       "  [('ni', 2), ('landara', 1), ('begitu', 1), ('emari', 1), ('hiri', 1)]),\n",
       " ('SERBIAN', [('i', 1), ('srce', 1), ('no', 1)]),\n",
       " ('NYANJA', [('yo', 1)]),\n",
       " ('SLOVENIAN', [('mimo', 1)]),\n",
       " ('ARABIC', [('حاتم', 1)]),\n",
       " ('TAGALOG', [('ako', 1), ('', 1), ('bebot', 1), (\"'di\", 1), (\"ito'y\", 1)]),\n",
       " ('ITALIAN', [('non', 21), ('e', 17), ('la', 17), ('io', 14), ('ho', 12)]),\n",
       " ('MALAGASY', [('oe', 1)]),\n",
       " ('CZECH', [('posso', 1)]),\n",
       " ('SWAHILI', [('so', 2), ('', 1)]),\n",
       " ('NORWEGIAN',\n",
       "  [('jeg', 2), ('utalige', 1), ('nå', 1), ('vestido', 1), ('the', 1)]),\n",
       " ('DUTCH', [('steek', 1), ('hey', 1), ('haisha', 1), ('from', 1)]),\n",
       " ('WELSH', [('sterling', 1)]),\n",
       " ('TURKISH', [('bilemem', 1), ('just', 1)])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First word occurrence of each song per language\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "first_count = sparkrdd \\\n",
    ".map(lambda perlink : (perlink[2], (perlink[1],perlink[4]))) \\\n",
    ".reduceByKey(lambda a,b : b) \\\n",
    ".map(lambda x : x[1] ) \\\n",
    ".map(lambda x : (x[0].strip(), x[1].strip())) \\\n",
    ".filter(lambda x: x[0].lower() != \"na\") \\\n",
    ".map( lambda x : (x[0].replace(\"\\n\",\"\"), [ re.sub(r\"[.,\\(\\)?!_\\-\\d]+\", '', x[1].lower()).split(\" \")[0]] )) \\\n",
    ".map(lambda x : (x[0], Counter(x[1]))) \\\n",
    ".reduceByKey(lambda a,b : Counter(dict((a + b).most_common(50)))) \\\n",
    ".map(lambda x : (x[0], x[1].most_common(5)))\n",
    "\n",
    "\n",
    "# .map( lambda x : (x[0], [word for word in x[1] if not word in get_stopwords(x[0].lower())][0] )) \\\n",
    "first_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENGLISH',\n",
       "  [('you', 3890), ('me', 3210), ('', 1234), ('yeah', 728), ('love', 728)]),\n",
       " ('SPANISH', [('amor', 131), ('x', 69), ('ti', 44), ('', 28), ('mi', 23)]),\n",
       " ('SWEDISH',\n",
       "  [('dig', 2), ('är', 2), ('hurra', 1), ('barn', 1), ('fernando', 1)]),\n",
       " ('FRENCH', [('moi', 14), ('pas', 13), ('toi', 10), ('', 5), ('you', 5)]),\n",
       " ('GALICIAN',\n",
       "  [('', 2), ('relato…', 2), ('nicacia', 1), ('outono', 1), ('claridade', 1)]),\n",
       " ('ESTONIAN',\n",
       "  [('jää', 2), ('lahuta', 1), ('õhu', 1), ('creepshow', 1), ('õitseb', 1)]),\n",
       " ('CATALAN',\n",
       "  [('azcapolanco', 1), ('amor', 1), ('barcelona', 1), ('aquí', 1), ('tu', 1)]),\n",
       " ('CEBUANO', [('tibum', 1)]),\n",
       " ('CROATIAN',\n",
       "  [('predaka', 1),\n",
       "   ('pobijediti', 1),\n",
       "   ('uklela', 1),\n",
       "   ('ref', 1),\n",
       "   ('vatre', 1)]),\n",
       " ('AFRIKAANS', [('yeah', 1)]),\n",
       " ('SLOVAK', [('uncool', 1)]),\n",
       " ('ROMANIAN', [('core', 1)]),\n",
       " ('BOSNIAN', [('jasno', 1)]),\n",
       " ('PORTUGUESE',\n",
       "  [('você', 2290), ('x', 1304), ('mim', 1287), ('', 848), ('amor', 841)]),\n",
       " ('POLISH', [('x', 1)]),\n",
       " ('HAITIAN_CREOLE',\n",
       "  [('aaaafade', 1), ('', 1), (\"dezame'w\", 1), ('fade]', 1), ('head', 1)]),\n",
       " ('KOREAN', [('you', 1)]),\n",
       " ('GANDA', [('ireoke', 1)]),\n",
       " ('HMONG', [('rua', 1)]),\n",
       " ('HUNGARIAN', [('virágom', 1)]),\n",
       " ('MALAY', [('one', 1), ('', 1), ('akbar', 1), ('tunjukkan', 1)]),\n",
       " ('SUNDANESE',\n",
       "  [('geoya', 1),\n",
       "   ('mayakovsky', 1),\n",
       "   ('smile', 1),\n",
       "   ('nanananananananananananana', 1),\n",
       "   ('life', 1)]),\n",
       " ('KURDISH', [(\"da'\", 1), ('carter', 1)]),\n",
       " ('INDONESIAN',\n",
       "  [('tonight', 1), ('lonely', 1), ('ser', 1), ('jawa', 1), ('berdua', 1)]),\n",
       " ('GERMAN', [('mich', 7), ('dich', 6), ('nicht', 5), ('', 4), ('mir', 4)]),\n",
       " ('KINYARWANDA', [('de', 3), ('jamun*', 2), ('kara', 2), ('e', 2), ('ka', 2)]),\n",
       " ('JAPANESE', [('彼のために泣いているんだね', 1), ('huh', 1)]),\n",
       " ('IRISH',\n",
       "  [('forlorn', 1), ('baby', 1), ('year', 1), ('come', 1), ('coule', 1)]),\n",
       " ('FINNISH',\n",
       "  [('oficial', 1), ('lapin', 1), ('ronaldorwp', 1), ('maan', 1), ('tuo', 1)]),\n",
       " ('ICELANDIC',\n",
       "  [('ber', 2), ('hopelandic', 2), ('þínum', 1), ('þokunni', 1), ('heima', 1)]),\n",
       " ('SESOTHO', [('ho', 1), ('okaý', 1)]),\n",
       " ('DANISH',\n",
       "  [('forever', 5), ('young', 1), ('grade', 1), ('man', 1), ('na', 1)]),\n",
       " ('RUSSIAN',\n",
       "  [('зарю', 1), ('amasupersuperstar', 1), ('забери', 1), ('мечтаю', 1)]),\n",
       " ('BASQUE',\n",
       "  [('hil', 2), ('aurrean', 1), ('gariena', 1), ('sartu', 1), ('ezetuko', 1)]),\n",
       " ('SERBIAN', [('vjeènim', 1), ('pobedu', 1), ('gueto', 1)]),\n",
       " ('NYANJA', [('love', 1)]),\n",
       " ('SLOVENIAN', [('dress', 1)]),\n",
       " ('ARABIC', [(\"lejo'\", 1)]),\n",
       " ('TAGALOG',\n",
       "  [('ko', 1), ('lang', 1), ('[x]', 1), ('oras', 1), ('madarama', 1)]),\n",
       " ('ITALIAN', [('te', 35), ('me', 23), ('noi', 13), ('tu', 13), ('mai', 9)]),\n",
       " ('MALAGASY', [('ohoh', 1)]),\n",
       " ('CZECH', [('senta', 1)]),\n",
       " ('SWAHILI', [('pouss', 2), ('day', 1)]),\n",
       " ('NORWEGIAN',\n",
       "  [('nature', 1),\n",
       "   ('horizon', 1),\n",
       "   ('dimensioner', 1),\n",
       "   ('estampado', 1),\n",
       "   ('herske', 1)]),\n",
       " ('DUTCH', [('sensimilla', 1), ('oh', 1), ('jogiya[x]', 1), ('u', 1)]),\n",
       " ('WELSH', [('mae', 1)]),\n",
       " ('TURKISH', [('geci', 1), ('love', 1)])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Last word occurrence of each song\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "last_count = sparkrdd \\\n",
    ".map(lambda perlink : (perlink[2], (perlink[1],perlink[4]))) \\\n",
    ".reduceByKey(lambda a,b : b) \\\n",
    ".map(lambda x : x[1] ) \\\n",
    ".map(lambda x : (x[0].strip(), x[1].strip())) \\\n",
    ".filter(lambda x: x[0].lower() != \"na\") \\\n",
    ".map( lambda x : (x[0].replace(\"\\n\",\"\"), [ re.sub(r\"[.,\\(\\)?!_\\-\\d]+\", '', x[1].lower()).split(\" \")[-1]] )) \\\n",
    ".map(lambda x : (x[0], Counter(x[1]))) \\\n",
    ".reduceByKey(lambda a,b : Counter(dict((a + b).most_common(50)))) \\\n",
    ".map(lambda x : (x[0], x[1].most_common(5)))\n",
    "\n",
    "\n",
    "# .map( lambda x : (x[0], [word for word in x[1] if not word in get_stopwords(x[0].lower())][:-1] )) \\\n",
    "last_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-QgxsF1M8yB"
   },
   "source": [
    "\n",
    "<p align=\"justify\">\n",
    "<hr style=\" border:none; height:2px;\">\n",
    " <font  size=\"3\" color='#91053d'>**DataFrames**</font>\n",
    "<hr style=\" border:none; height:2px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBkYMzzIM8yB",
    "outputId": "2f6a99b7-c42d-4401-9a7b-96628f3a0472"
   },
   "source": [
    "# Part 2 - Dataframes\n",
    "\n",
    "In this part you can use Pandas Dataframes or  Spark Dataframes.  I suggest to use a Spark Dataframe\n",
    "end exploit the Pandas functionalities as we have seen in the 2nd assignment. Download the two available datasets at the link:\n",
    "\n",
    "https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres\n",
    "\n",
    "You can find two .cvs files: \n",
    "\n",
    "* artists-data.csv\n",
    "\n",
    "* lyrics-data.csv\n",
    "\n",
    "\n",
    "# Import artist data.\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The artist data in the .csv file can be stored in a dataframe. \n",
    "    \n",
    "Each row of the .csv file describes an artist and the columns represent the following data:\n",
    "    \n",
    "* Artist - The artist's name\n",
    "* Popularity - Popularity score at the date of scrapping\n",
    "* ALink - The link to the artist's page\n",
    "* AGenre - Primary musical genre of the artist\n",
    "* AGenres - A list (pay attention to the format) of genres the artist fits in\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "# Import song's lyrics data.\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "Each row of the .csv file describes a lyric and the columns represent the following data:\n",
    "    \n",
    "* ALink - The link to the webpage of the artist\n",
    "* SLink - The link to the webpage of the song\n",
    "* Idiom - The idiom of the lyric\n",
    "* Lyric - The lyrics\n",
    "* SName - The name of the song\n",
    "\n",
    "    \n",
    "\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistdf = spark.read.option(\"header\",\"true\").csv(\"./artists-data.csv\")\n",
    "lyricsdf = spark.read.option(\"header\",\"true\").csv(\"./lyrics-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQn1a3PqM80l"
   },
   "source": [
    "#  Exercise 4 - (3 points) - Artist's genre\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Provide a program that finds the artists for which the genre is not specified.\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0liD2NOVM8yE",
    "outputId": "65272f02-1a32-4761-dcc4-53c39179d391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "+------+-----+----------+----+-----+------+\n",
      "|Artist|Songs|Popularity|Link|Genre|Genres|\n",
      "+------+-----+----------+----+-----+------+\n",
      "+------+-----+----------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# artistdf.show()\n",
    "#artists for which genre is not specified\n",
    "noGenreArtist = artistdf.filter(artistdf.Genres.isNull() | artistdf.Genre.isNull())\n",
    "print(noGenreArtist.count())\n",
    "noGenreArtist.show()\n",
    "\n",
    "#there are no artists for whicch genre is not specified in Genre and Genres column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 5 - (3 points) - Duplicates\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Provide a program that removes the duplicates in the artists (also in this case the URL is the key).\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3242\n",
      "2940\n",
      "2940\n"
     ]
    }
   ],
   "source": [
    "### Write here your code\n",
    "\n",
    "print(artistdf.count())\n",
    "print(artistdf.select('Link').distinct().count())\n",
    "\n",
    "remove_duplicate_artists = artistdf.dropDuplicates(['Link'])\n",
    "\n",
    "#the count of artists dataframe after removing duplicates by link\n",
    "print(remove_duplicate_artists.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 6 - (4 points)\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Provide a program that using dataframe return the 100 most popular artists and the lyrics of their songs.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular Artists count 100\n",
      "Popular Songs/Lyrics count 21810\n",
      "+-------------+---------------+--------------------+\n",
      "|       Artist|           Link|               Lyric|\n",
      "+-------------+---------------+--------------------+\n",
      "|4 Non Blondes|/4-non-blondes/|Twenty-five years...|\n",
      "|4 Non Blondes|/4-non-blondes/|Starry night brin...|\n",
      "|4 Non Blondes|/4-non-blondes/|Every time you wa...|\n",
      "|4 Non Blondes|/4-non-blondes/|What ya gonna do ...|\n",
      "|4 Non Blondes|/4-non-blondes/|How can you tell,...|\n",
      "|4 Non Blondes|/4-non-blondes/|Bless the beasts ...|\n",
      "|4 Non Blondes|/4-non-blondes/|\"I'm looking outs...|\n",
      "|4 Non Blondes|/4-non-blondes/|Here i am and i'm...|\n",
      "|4 Non Blondes|/4-non-blondes/|\"Substitute my gl...|\n",
      "|4 Non Blondes|/4-non-blondes/|Bless the beasts ...|\n",
      "|4 Non Blondes|/4-non-blondes/|How can you tell,...|\n",
      "|4 Non Blondes|/4-non-blondes/|\"I'm looking outs...|\n",
      "|4 Non Blondes|/4-non-blondes/|What a wonderful ...|\n",
      "|4 Non Blondes|/4-non-blondes/|Ah-hah!. Woo!. Ah...|\n",
      "|4 Non Blondes|/4-non-blondes/|Fried my brain be...|\n",
      "|4 Non Blondes|/4-non-blondes/|\"Walkin' in the p...|\n",
      "|4 Non Blondes|/4-non-blondes/|\"Substitute my gl...|\n",
      "|4 Non Blondes|/4-non-blondes/|When I wake up in...|\n",
      "|4 Non Blondes|/4-non-blondes/|\"Stumbled my way ...|\n",
      "|4 Non Blondes|/4-non-blondes/|Every time you wa...|\n",
      "+-------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "popular_artists = artistdf.sort(artistdf.Popularity.desc()).limit(100)\n",
    "\n",
    "popular_artists_lyrics = popular_artists.join(lyricsdf, popular_artists.Link == lyricsdf.ALink, \"inner\")\n",
    "popular_artists_lyrics = popular_artists_lyrics.select(\"Artist\",\"Link\",\"Lyric\")\n",
    "\n",
    "print(\"Popular Artists count\",popular_artists.count())\n",
    "print(\"Popular Songs/Lyrics count\",popular_artists_lyrics.count())\n",
    "\n",
    "popular_artists_lyrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DatBVcKUM8yn"
   },
   "source": [
    "# 2 - Bonus \n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Using the approach you prefer (just Dataframes, hybrid approach)  :\n",
    "    \n",
    "* the 10 most common words in the lyrics of each artist\n",
    "* the 10 most common words for each genre. For this question we can use the primary genre of the artist.\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3EIzkQFM8yo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "ass4-map-reduce-spark-colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "BE4-Spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
